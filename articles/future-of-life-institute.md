# Future of Life Institute

The **Future of Life Institute (FLI)** is a nonprofit organization founded in March 2014 that works to steer transformative technologies toward benefiting life and away from large-scale risks. Headquartered in Campbell, California, with offices in Brussels, London, and Washington D.C., FLI focuses primarily on [[ai-safety|existential risks from advanced artificial intelligence]], while also addressing dangers from biotechnology and nuclear weapons.

## Overview

FLI's stated mission is preserving the future of life by ensuring that powerful technologies are properly managed. The organization operates through three main activities: grantmaking to support safety research, educational outreach to raise public awareness, and policy advocacy within governments and international institutions.

FLI has become one of the most prominent voices in AI safety discourse, known for organizing high-profile open letters, funding foundational research, and engaging directly with policymakers in the United States, European Union, and United Nations.

## History

### Founding

FLI was founded in March 2014 by:
- **Max Tegmark**: MIT cosmologist and physicist, current president
- **Jaan Tallinn**: Co-founder of Skype and early AI safety advocate
- **Anthony Aguirre**: UCSC physicist and cosmologist
- **Viktoriya Krakovna**: DeepMind research scientist
- **Meia Chita-Tegmark**: Tufts University researcher

### Early Years

FLI held its opening event at MIT in 2014—a panel on "The Future of Technology: Benefits and Risks" moderated by actor Alan Alda. In 2015, the organization received a $10 million donation from Elon Musk, enabling its first major grant program. That round awarded $7 million to 37 AI safety research projects, helping establish the technical field.

The 2017 Asilomar Conference on Beneficial AI, sponsored by FLI, brought together over 100 thought leaders who formulated influential principles for responsible AI development, including commitments to avoid "corner-cutting on safety standards" in competitive races.

### Major Initiatives

**2017**: FLI released *Slaughterbots*, a viral short film depicting the dangers of autonomous weapons, followed by a sequel in 2021.

**2018**: Coordinated a letter calling for laws against lethal autonomous weapons, signed by Elon Musk, Demis Hassabis, and other AI leaders.

**2021**: Received $25 million from Ethereum co-founder Vitalik Buterin for expanded grantmaking.

## Open Letters on AI

### Pause Giant AI Experiments (March 2023)

Following the release of GPT-4, FLI published a letter calling for a six-month pause on training AI systems "more powerful than GPT-4." The letter warned of an "out-of-control race" among AI labs and called for using pause time to establish safety frameworks. Prominent signatories included Elon Musk, Steve Wozniak, Yoshua Bengio, and Yuval Noah Harari.

The letter generated significant debate. Some signatories like Bengio explicitly endorsed the pause proposal; others like Gary Marcus signed for the "spirit" while noting imperfections. Critics, including the authors of the "Stochastic Parrots" paper, argued the letter prioritized speculative future risks over present harms.

### Statement on Superintelligence (October 2025)

FLI's most recent major letter calls for a prohibition on developing [[superintelligence]] "not lifted before there is broad scientific consensus that it will be done safely and controllably, and strong public buy-in." The statement gathered over 110,000 signatures including:
- Nobel laureates Geoffrey Hinton, Daron Acemoglu, and Frank Wilczek
- AI pioneer Yoshua Bengio
- Public figures including Prince Harry and Meghan Markle, Stephen Fry, Will.i.am, and Joseph Gordon-Levitt
- Former National Security Advisor Susan Rice
- OpenAI researcher Leo Gao

FLI director Anthony Aguirre stated that "time is running out," predicting superintelligence could arrive within one to two years. Polling released with the letter showed 64% of Americans agreed superintelligence "shouldn't be developed until it's provably safe and controllable."

## Policy Work

FLI maintains active policy engagement:

- **U.S. Congress**: In 2023, Senate Majority Leader Chuck Schumer invited FLI to brief selected senators on AI regulation
- **European Union**: Successfully advocated for including general-purpose AI systems like GPT-4 in the EU AI Act's scope
- **United Nations**: Advocates for treaties on autonomous weapons; supported the Treaty on the Prohibition of Nuclear Weapons
- **AI Action Plan**: Submitted detailed recommendations to the Trump administration's 2025 AI Action Plan

## Advisors and Leadership

FLI's advisory board has included computer scientists Stuart Russell and Francesca Rossi, biologist George Church, physicist Frank Wilczek, entrepreneur Elon Musk, and the late Stephen Hawking. Actors Alan Alda and Morgan Freeman serve as science communication advisors.

Max Tegmark serves as president, with the organization employing 20+ staff members across policy, outreach, and grantmaking teams.

## Focus Areas

While primarily known for AI safety work, FLI addresses three focus areas:

1. **Artificial Intelligence**: Ensuring AI benefits humanity rather than destabilizing society, enabling terrorism, or replacing humans
2. **Biotechnology**: Managing advances that could enable engineered pandemics or novel biological weapons
3. **Nuclear Weapons**: Preventing catastrophic nuclear conflict and potential nuclear winter

## Controversies

In January 2023, Swedish magazine Expo reported that FLI had offered a $100,000 grant to a foundation associated with Nya Dagbladet, a Swedish far-right publication. Tegmark stated the grant was immediately revoked upon discovering the publication's positions during due diligence.

## Current State (2025-2026)

FLI continues as a leading voice in AI safety advocacy. Recent work includes the superintelligence prohibition campaign, recommendations for the US AI Action Plan, and ongoing research on AI's potential impact on power distribution and democratic institutions. The organization has helped shift discourse such that major AI company leaders now publicly acknowledge existential risks—a position that seemed fringe when FLI was founded.

## See Also

- [[ai-safety]]
- [[alignment-problem]]
- [[ai-governance]]
- [[center-for-ai-safety]]
- [[pause-ai]]

## References

1. Wikipedia. "Future of Life Institute." Accessed February 2026.
2. Future of Life Institute. "About Us." futureoflife.org. Accessed February 2026.
3. "Pause Giant AI Experiments: An Open Letter." Future of Life Institute, March 2023.
4. "Statement on Superintelligence." Future of Life Institute, October 2025.
