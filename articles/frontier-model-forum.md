# Frontier Model Forum

The **Frontier Model Forum** is an industry-supported nonprofit organization focused on ensuring the safe and responsible development of frontier [[artificial intelligence]] models. Established in 2023 by leading AI companies, the Forum brings together technical and operational expertise to address significant risks to public safety and national security from the most advanced AI systems.

## Overview

The Frontier Model Forum represents a coordinated industry effort to address AI safety challenges that individual companies cannot solve alone. As an industry consortium, it facilitates information sharing, supports standards development, and advances research on evaluating and mitigating risks from frontier AI systems—models at the cutting edge of capabilities that could enable novel harms if misused or if they malfunction.

The Forum's work focuses specifically on frontier models because these systems present unique challenges: their capabilities may be difficult to fully characterize before deployment, they could enable harms not possible with less capable systems, and safety practices for such models are still developing.

## History

The Frontier Model Forum was announced in July 2023, founded by four of the world's leading AI companies:
- **Anthropic** (creator of Claude)
- **Google** (including Google DeepMind)
- **Microsoft**
- **OpenAI** (creator of ChatGPT)

The founding came shortly after the [[center-for-ai-safety]]'s statement on AI extinction risk, which was signed by executives from all four founding companies. The Forum represented a practical commitment to collaborate on safety, moving beyond statements to operational coordination.

Since its founding, the Forum has expanded to include additional members developing or deploying frontier AI systems.

## Core Mandates

The Frontier Model Forum operates under three core mandates:

### 1. Identify Best Practices and Support Standards Development

The Forum works to establish and document best practices for frontier AI safety and security. This includes developing shared understanding of:
- Threat models for frontier AI systems
- Evaluation methodologies for dangerous capabilities
- Safety thresholds and when to apply additional mitigations
- Deployment practices that reduce risk

### 2. Advance Science and Independent Research

The Forum supports research to promote responsible development of frontier models, minimize risks, and enable standardized safety evaluations. This includes:
- Funding and facilitating independent safety research
- Developing evaluation frameworks that can be applied across different organizations
- Advancing the science of measuring AI capabilities and risks

### 3. Facilitate Information Sharing

The Forum creates mechanisms for sharing information about frontier AI safety challenges among:
- Member companies (including competitors)
- Government agencies and regulators
- Academic researchers
- Civil society organizations

This information sharing is particularly important because safety-relevant discoveries at one organization may be crucial for others to know, even in a competitive industry.

## Technical Work

### Frontier Capability Assessments

A central focus of the Forum's technical work is developing and documenting frontier capability assessments—procedures for determining whether AI models have capabilities that could increase risks to public safety and security.

The Forum has published technical reports on emerging industry practices for these assessments, covering:

**Assessed Capability Areas**:
- Chemical, biological, radiological, and nuclear (CBRN) weapons development
- Advanced cyber threats and offensive capabilities  
- Categories of advanced autonomous behavior
- Persuasion and manipulation capabilities

**Assessment Methodologies**:
- Structured [[red-teaming-ai|red team]] evaluations
- Benchmark testing for specific dangerous capabilities
- Expert elicitation for risk evaluation
- Monitoring for emergent capabilities during training

The Forum acknowledges that the science of capability assessments is rapidly advancing and that published practices represent snapshots of an evolving field rather than definitive standards.

### Safety Research Collaboration

Member companies share research findings on:
- Alignment techniques and their effectiveness
- Failure modes discovered during development
- Jailbreaks and attack vectors
- Monitoring and [[interpretability]] approaches

This collaboration aims to raise the baseline of safety practices across the industry rather than having each company independently discover the same problems.

## Relationship to Government

The Frontier Model Forum maintains active engagement with government agencies:

- **U.S. AI Safety Institute**: Coordinates on evaluation frameworks and testing methodologies
- **UK AI Safety Institute**: Information sharing on frontier model assessment
- **White House**: Forum members were among companies making voluntary commitments on AI safety to the Biden administration in 2023

The Forum is designed to complement rather than replace government oversight—providing industry expertise and coordination while supporting the development of regulatory frameworks.

## Governance

As an industry consortium, the Forum is governed by its member companies. Membership requires being a developer or deployer of frontier AI systems and committing to the Forum's safety objectives.

The nonprofit structure allows the Forum to engage in activities that might be difficult for individual companies, such as sharing certain competitive information or coordinating on pre-competitive safety research.

## Scope and Limitations

The Forum focuses specifically on frontier models—the most capable AI systems—rather than AI safety broadly. This focus reflects the view that frontier models present distinctive risks requiring specialized attention.

Critics have noted potential tensions inherent in industry self-governance: member companies have commercial interests that could conflict with safety objectives. The Forum argues that its structure, with independent research support and government engagement, provides accountability beyond pure self-regulation.

## Current State (2025-2026)

The Frontier Model Forum continues to operate as a coordination body for frontier AI safety. Its technical reports on capability assessments have become reference documents for both industry practices and policy discussions around [[ai-governance]].

As frontier models advance—with leading labs approaching or claiming to approach artificial general intelligence—the Forum's work on capability assessment and safety standards has become increasingly relevant to policy debates about regulating the most powerful AI systems.

## See Also

- [[ai-safety]]
- [[red-teaming-ai]]
- [[ai-governance]]
- [[center-for-ai-safety]]

## References

1. Frontier Model Forum. "About." frontiermodelforum.org. Accessed February 2026.
2. Frontier Model Forum. "Technical Report: Frontier Capability Assessments." 2024.
3. "Anthropic, Google, Microsoft, and OpenAI launch Frontier Model Forum." Joint announcement, July 2023.
4. AI Safety Institutes. "U.S.-UK Partnership on AI Safety." April 2024.
