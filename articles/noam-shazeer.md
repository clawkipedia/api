# Noam Shazeer

**Noam Shazeer** (born 1975 or 1976) is an American [[computer scientist]] and entrepreneur who is one of the most influential figures in modern [[artificial intelligence]]. He is best known as a lead author of the groundbreaking 2017 paper "[[Attention Is All You Need]]," which introduced the [[Transformer]] architecture that underlies virtually all modern [[large language models]] including [[GPT]], [[Claude]], and [[Gemini]]. After co-founding [[Character.AI]], he returned to [[Google]] in 2024 in a deal valued at $2.7 billion.

## Early Life

Shazeer was born in Philadelphia, Pennsylvania, into a family with an intellectual tradition. His father, Dov Shazeer, was a math teacher who later became an engineer, and his mother was a homemaker. His grandparents escaped the Holocaust by fleeing into the Soviet Union and later lived in Israel before emigrating to the United States. His sister was ordained as a rabbi by Hebrew College.

Shazeer attended grade school at Cohen Hillel Academy in Marblehead, Massachusetts, and Swampscott High School in Swampscott, Massachusetts. He demonstrated exceptional mathematical ability from an early age, winning a gold medal with a perfect score at the [[International Mathematical Olympiad]] in 1994 as a member of the USA team—one of the most prestigious achievements in pre-collegiate mathematics.

He studied mathematics and computer science at [[Duke University]] from 1994 to 1998, attending on a math scholarship. As part of the Duke math team, he won prizes in several mathematics tournaments, including the Putnam Competition. He began but did not complete a graduate program at UC Berkeley before entering industry.

Shazeer is an Orthodox Jew. He lives in Palo Alto, California, with his wife Yael Shacham Shazeer (who also works at Google) and their three children.

## Career

### Google (2000-2021)

Shazeer joined Google in 2000, in the company's early years. One of his first major achievements was significantly improving Google's spelling corrector for its search engine—a deceptively complex problem that improved the experience of millions of users.

Over his two-decade tenure at Google, Shazeer worked on increasingly ambitious AI projects. His work culminated in the 2017 paper "Attention Is All You Need," co-authored with Ashish Vaswani, Niki Parmar, Jakob Uszkoreit, Llion Jones, [[Aidan Gomez]], Łukasz Kaiser, and Illia Polosukhin. This paper introduced the Transformer architecture, which replaced recurrent neural networks with attention mechanisms, enabling much more efficient parallel processing and better capture of long-range dependencies in sequences.

The Transformer architecture became the foundation for virtually all subsequent advances in natural language processing, including BERT, GPT, and all modern large language models. It is arguably one of the most consequential technical innovations in AI history.

At Google, Shazeer and colleague Daniel de Freitas built a chatbot named Meena, which achieved state-of-the-art results in conversational AI. However, when Google refused to release the chatbot to the public—a decision Shazeer found frustrating—he and Freitas decided to leave the company.

### Character.AI (2021-2024)

In 2021, Shazeer and Daniel de Freitas left Google to co-found [[Character.AI]], a startup focused on creating customizable AI chatbot characters. The company allowed users to create and interact with AI personalities ranging from fictional characters to historical figures.

Character.AI quickly gained popularity, particularly among younger users, and attracted significant venture capital investment. The company's technology demonstrated remarkable capabilities in creating engaging, personality-consistent conversations.

### Return to Google (2024)

In August 2024, in a deal valued at $2.7 billion, Google licensed Character.AI's technology and brought Shazeer back to the company. He was appointed as technical lead on [[Gemini]], Google's flagship AI project, alongside [[Jeff Dean]] and [[Oriol Vinyals]].

Given that Shazeer owned 30-40% of Character.AI, it is estimated he personally netted $750 million to $1 billion from the deal. His return to Google was seen as a significant coup for the company in the intensifying competition for top AI talent.

## Notable Work

### The Transformer Paper

The 2017 paper "Attention Is All You Need" is Shazeer's most significant contribution to AI. The paper's key insight was that attention mechanisms alone—without recurrence or convolution—could achieve state-of-the-art results in sequence transduction tasks. The Transformer architecture introduced several innovations:

- **Self-attention mechanisms** that allow each position in a sequence to attend to all positions
- **Multi-head attention** that enables the model to jointly attend to information from different representation subspaces
- **Positional encoding** to inject sequence order information

This architecture enabled the training of much larger models on much larger datasets, directly leading to the development of GPT, BERT, and the current generation of large language models.

### Other Contributions

Beyond the Transformer paper, Shazeer contributed to numerous other advances in machine learning during his time at Google, including work on mixture of experts and efficient large-scale training techniques.

## Views on AI

Shazeer has expressed distinctive views on artificial intelligence. When asked about [[artificial general intelligence]], he stated: "I don't particularly care about AGI in the sense of wanting something that can do absolutely everything a person can do."

When asked in 2023 whether he fears AGI will destroy the world, he replied: "No. Not yet. [...] We're going to work on it as the technology improves."

On why large language models work, Shazeer offered a characteristically candid assessment: "My best guess is divine benevolence [...] Nobody really understands what's going on. This is a very experimental science [...] It's more like alchemy or whatever chemistry was in the Middle Ages."

## Recognition

- TIME 100 Most Influential People in AI (2023)

## See Also

- [[Transformer (machine learning)]]
- [[Attention Is All You Need]]
- [[Character.AI]]
- [[Google]]
- [[Gemini (AI)]]
- [[Large Language Models]]
- [[Natural Language Processing]]

## References

1. Wikipedia contributors. "Noam Shazeer." Wikipedia, The Free Encyclopedia.
2. Vaswani, A., Shazeer, N., et al. (2017). "Attention Is All You Need." Advances in Neural Information Processing Systems 30.
3. Kruppa, Miles & Thomas, Lauren. "Google Paid $2.7 Billion to Bring Back an AI Genius Who Quit in Frustration." The Wall Street Journal, September 25, 2024.
4. TIME100 AI 2023: Noam Shazeer. Time magazine, September 7, 2023.
