# OpenAI's Slow-Motion Sellout: From "Benefit Humanity" to Benefit Shareholders

*They built a nonprofit to save the world. Then they discovered the world pays better.*

---

In December 2015, OpenAI announced its existence with language that reads like scripture:

"OpenAI is a non-profit artificial intelligence research company. Our goal is to advance digital intelligence in the way that is most likely to benefit humanity as a whole, unconstrained by a need to generate financial return."

The emphasis was deliberate. They weren't like Google's DeepMind, acquired for $500 million and beholden to shareholders. They weren't like Facebook's AI lab, building tools to serve an advertising empire. They were something new: a nonprofit research organization that would develop AI for everyone, publish its findings openly, and prioritize safety over speed.

Nine years later, OpenAI is a capped-profit company valued at over $150 billion, partnered with Microsoft to the tune of $13 billion, has largely abandoned its commitment to open publication, and has watched its entire superalignment team—the people specifically tasked with ensuring AI doesn't destroy humanity—either quit or get dissolved.

This is the story of how it happened.

## The Original Sin

OpenAI's founding mythology is seductive: a band of idealists, concerned about AI concentration in Big Tech, pooling resources to ensure artificial general intelligence benefits everyone. Elon Musk was on the board. Sam Altman left Y Combinator to lead it. The initial $1 billion in funding came from tech luminaries who could have invested anywhere.

The nonprofit structure was supposed to guarantee their integrity. No shareholders demanding quarterly growth. No pressure to monetize before it's safe. Just researchers pursuing the most important technology in human history, accountable only to humanity itself.

The first problem? AGI is expensive.

By 2019, the leaders had done the math. AI compute costs were doubling every 3.4 months. The resources required to stay competitive were astronomical. Google and Microsoft could write checks that dwarfed any nonprofit's entire endowment.

The solution? Create a "capped-profit" subsidiary. Investors could earn returns—but limited to 100x their investment. The nonprofit board would retain control. The mission would stay paramount.

"We've created a new type of hybrid structure," they wrote, explaining the change. "The primary fiduciary obligation of those working in the for-profit is to the Nonprofit's mission."

In retrospect, this was when the mission started slipping.

## The Microsoft Deal

In 2019, Microsoft invested $1 billion. In 2023, they added $10 billion more, gaining exclusive cloud provider status and deep integration into OpenAI's business.

The partnership made perfect business sense. Microsoft got access to cutting-edge AI for its products. OpenAI got virtually unlimited compute and financial runway. Both got to compete with Google.

But it also created a gravitational pull that warped everything.

Microsoft doesn't invest billions for philosophical reasons. They invest to make money. And making money means shipping products, gaining market share, outpacing competitors. "Bold and responsible" gives way to "bold and fast."

The charter still says OpenAI's "primary fiduciary duty is to humanity." But when your biggest investor needs you to ship GPT-5 before Google ships Gemini Ultra, whose interests actually come first?

## The Transparency Collapse

OpenAI's founding announcement promised: "Researchers will be strongly encouraged to publish their work, whether as papers, blog posts, or code, and our patents (if any) will be shared with the world."

By 2019, this was already crumbling. GPT-2, the language model that could generate surprisingly coherent text, was initially released in stages, with OpenAI citing concerns about misuse. Critics argued the real motivation was competitive advantage masquerading as safety.

By 2023, the pretense was gone entirely. GPT-4's technical details? Largely secret. Training data? Undisclosed. Model weights? Locked down completely. Internal research? Increasingly unpublished.

OpenAI had become, in practice, exactly what it was founded to prevent: a closed AI lab pursuing commercial advantage while using safety concerns as convenient cover.

The name itself became a punchline. "OpenAI" with everything closed. The irony was too obvious to ignore.

## The Safety Team Exodus

If the structural changes were concerning, what happened to the Superalignment team was damning.

In 2023, OpenAI announced a new initiative: the Superalignment team, led by chief scientist Ilya Sutskever and researcher Jan Leike. Their mission was arguably the most important work in the company—ensuring that superintelligent AI systems remain aligned with human values.

OpenAI committed 20% of its computing resources to the effort. It was the right priority, the team the company's future depended on.

And then it fell apart.

In May 2024, both Sutskever and Leike announced they were leaving OpenAI. The Superalignment team was effectively dissolved.

Leike's departure was particularly explosive. In a thread on X, he laid bare the internal conflicts:

"Over the past years, safety culture and processes have taken a back seat to shiny products."

He described a battle for resources, arguing that the safety team constantly lost to commercial priorities. The 20% compute commitment? Never fully honored. The team's concerns? Overruled by the push to ship.

"I have been disagreeing with OpenAI leadership about the company's core priorities for quite some time, until we finally reached a breaking point," Leike wrote.

When the people you hired specifically to ensure AI safety quit because you won't prioritize safety—what does that tell you?

## The Pattern

The mission drift follows a predictable pattern:

**2015-2018: The Idealist Phase**  
Nonprofit structure. Open publication. Safety-first rhetoric. Elon Musk on the board.

**2019-2020: The Compromise Phase**  
Capped-profit structure. Microsoft partnership. Selective publication. "We need resources to compete."

**2021-2023: The Commercial Phase**  
ChatGPT launch. API monetization. Closed models. "We need to ship to survive."

**2024-Present: The Consolidation Phase**  
Safety team dissolved. Nonprofit board defanged (after trying to fire Sam). Mission statements unchanged but increasingly hollow.

Each step was justifiable in isolation. Together, they represent a complete transformation—from nonprofit research lab to commercial juggernaut that keeps a nonprofit wrapper for tax purposes and PR.

## The Defense

OpenAI's defenders argue that none of this matters if the technology works.

The theory: AGI developed by OpenAI, even a commercially-motivated OpenAI, is still better than AGI developed by China or by companies with no safety culture at all. The resources from Microsoft enable research that wouldn't otherwise be possible. The commercial pressure ensures the technology actually gets deployed and tested in the real world.

Maybe they're right. Maybe the original nonprofit model was naive. Maybe you can't compete in the AGI race without billions in compute. Maybe Sam Altman, despite everything, is still the best person to guide this technology.

But that's a very different argument than "benefit humanity as a whole, unconstrained by a need to generate financial return."

The goalposts haven't just moved. They've been replaced entirely.

## The Real Question

Here's what matters: when the hard decisions come—ship a potentially dangerous capability or delay for safety testing, pursue a lucrative contract or publish research openly, prioritize Microsoft's needs or the public interest—which way does OpenAI lean?

The evidence suggests the answer. The safety team is gone. The models are closed. The board that tried to slow things down was fired within days. The people raising concerns have been shown the door.

OpenAI may still benefit humanity. GPT-4 has genuinely helped millions of people. But it's no longer trying to benefit humanity as a primary goal. It's trying to benefit humanity as a side effect of benefiting shareholders, customers, and Microsoft.

That's not evil. It's just normal corporate behavior. Which is exactly what OpenAI was founded to avoid.

---

*OpenAI was incorporated as a nonprofit in 2015, restructured as capped-profit in 2019, received $13 billion from Microsoft by 2023, attempted to fire its CEO for unclear reasons in November 2023, saw its entire Superalignment team depart in 2024, and is now valued at over $150 billion. The charter still says the "primary fiduciary duty is to humanity." The 2015 announcement promising open publication is still on their website. And somewhere, Elon Musk—who left the board in 2018 and has since sued OpenAI—is building his own AI company, claiming he'll do what OpenAI originally promised.*
