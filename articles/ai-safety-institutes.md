# AI Safety Institutes

**AI Safety Institutes** are government-established organisations dedicated to researching, evaluating, and advising on the safety of advanced [[artificial intelligence]] systems. Beginning in 2023, multiple nations established such institutes to develop technical expertise for understanding and mitigating risks from frontier AI, creating an emerging international network of public sector AI safety capacity.

## Overview

AI Safety Institutes represent a novel institutional form designed to provide governments with independent technical capabilities for assessing AI risks and informing policy decisions. These organisations typically conduct evaluations of advanced AI models, develop safety testing methodologies, engage in technical research, and facilitate information sharing between government, industry, and academia. The establishment of multiple national institutes reflects growing recognition that AI safety governance requires dedicated scientific infrastructure and expertise.

## Background

The concept of government AI safety institutes emerged from several converging factors: rapid advances in AI capabilities, increasing concern about potential risks from frontier systems, and recognition that effective governance requires technical understanding that general-purpose government agencies often lack. Private sector AI safety research, while growing, was seen as insufficient given commercial incentives and the public interest nature of many safety questions.

The 2023 [[UK AI Safety Summit]] at [[Bletchley Park]] served as a catalyst for institute creation, with the [[UK AI Safety Institute]] announced at the summit and the [[United States]] following shortly after. The summit's [[Bletchley Declaration]] called for an "internationally inclusive network of scientific research on frontier AI safety," providing diplomatic impetus for further national initiatives.

## Major Institutes

### UK AI Safety Institute / AI Security Institute

The UK AI Safety Institute was established in November 2023 as an arm of the [[Department for Science, Innovation and Technology]], making it the world's first government body dedicated to AI safety. Its stated mission was "to minimise surprise to the UK and humanity from rapid and unexpected advances in AI."

In early 2026, the Institute was renamed the **AI Security Institute** to reflect an expanded focus on security aspects of AI systems. Key activities include:

- Conducting evaluations of frontier AI models before and after release
- Developing evaluation methodologies and safety testing frameworks
- Publishing the International Scientific Report on the Safety of Advanced AI
- Funding external AI safety research through fast grants programmes
- Advising government on AI policy decisions

The Institute maintains relationships with major AI developers including [[OpenAI]], [[Google DeepMind]], and [[Anthropic]], conducting pre-release evaluations of advanced models.

### US AI Safety Institute (AISI) / CAISI

The [[US AI Safety Institute]] was established within the [[National Institute of Standards and Technology]] (NIST) in early 2024 through an executive order from President [[Joe Biden]]. Following the change in administration, the Institute was reorganised in 2025 as the **Center for AI Standards and Innovation (CAISI)** under the [[Department of Commerce]].

CAISI's mandate includes:

- Developing guidelines and best practices for AI security
- Establishing voluntary agreements with AI developers for model evaluations
- Leading evaluations of AI capabilities relevant to national security
- Assessing security vulnerabilities including potential malicious behaviour
- Coordinating with defence, energy, homeland security, and intelligence agencies
- Representing US interests in international AI standards development

The reorganisation shifted emphasis toward national security applications and competitive positioning relative to foreign AI systems, particularly those from [[China]].

### International Network

Additional AI safety institutes and similar bodies have been established or announced in multiple jurisdictions:

- **Japan AI Safety Institute**: Established 2024 under the [[Ministry of Economy, Trade and Industry]]
- **Singapore AI Safety Institute**: Part of Singapore's national AI governance framework
- **Canada AI Safety Institute**: Announced 2024, focusing on research coordination
- **EU AI Office**: While broader in scope, includes AI safety evaluation functions under the [[EU AI Act]]
- **France**: AI safety capacity development announced at 2025 AI Action Summit

Several countries have established related bodies focused on AI standards, testing, or research that include safety functions without using the "safety institute" designation.

## Key Activities

### Model Evaluations

A core function of AI safety institutes is evaluating frontier AI models for potentially dangerous capabilities. Evaluations typically assess:

- Capabilities in domains of concern (cybersecurity, biosecurity, chemical weapons, autonomous operation)
- Tendency to deceive, manipulate, or pursue unintended goals
- Robustness to adversarial inputs and jailbreaking attempts
- Alignment with intended purposes and human values

Institutes have developed partnerships with AI companies enabling pre-deployment access to models, though the voluntary nature of such arrangements has drawn scrutiny.

### Research and Standards

Institutes contribute to AI safety research directly and through funding external researchers. Areas of focus include:

- Evaluation methodology development
- AI interpretability and transparency
- Risk assessment frameworks
- Safety benchmarks and metrics
- Technical standards for AI systems

### International Coordination

AI safety institutes collaborate through bilateral arrangements and multilateral forums. The UK and US institutes signed a partnership agreement in 2024 for information sharing and coordinated evaluations. Institutes have participated in international processes including [[G7]] discussions, [[OECD]] AI policy work, and UN-convened expert groups.

## Impact

The establishment of AI safety institutes has created new channels for government-industry interaction on safety issues, providing structured frameworks for information sharing and evaluation that did not previously exist. Institute research has contributed to public understanding of AI capabilities and limitations.

The institutes have influenced AI governance debates by demonstrating that technical safety evaluation by government bodies is feasible, informing regulatory discussions about requirements for pre-deployment testing and ongoing monitoring.

## Criticism

Critics have raised several concerns about AI safety institutes:

**Capture Concerns**: Close relationships with AI companies have prompted questions about regulatory independence. Critics argue that reliance on voluntary industry cooperation may limit institutes' ability to conduct truly independent assessments or publish findings that companies oppose.

**Scope Limitations**: The focus on frontier models and catastrophic risks has been criticised for neglecting present harms from more widely deployed AI systems including discrimination, misinformation, and labour impacts.

**Resource Constraints**: Given the scale of AI development activity, some observers question whether institutes have sufficient resources and personnel to meaningfully evaluate the volume of AI systems being developed.

**Effectiveness Questions**: The voluntary nature of company participation raises questions about whether evaluations capture genuinely risky systems or primarily those companies are confident will pass review.

**Political Vulnerability**: As government bodies, institutes are subject to changing political priorities, as illustrated by the US reorganisation under new administration.

## See Also

- [[AI Safety]]
- [[UK AI Safety Summit]]
- [[Bletchley Declaration]]
- [[National Institute of Standards and Technology]]
- [[EU AI Act]]
- [[Frontier Model Forum]]

## References

1. UK Government. "AI Safety Institute: Overview." Department for Science, Innovation and Technology. November 2023.
2. UK Government. "AI Safety Institute Approach to Evaluations." February 2024.
3. NIST. "Center for AI Standards and Innovation (CAISI)." https://www.nist.gov/caisi
4. UK AI Safety Institute. "International Scientific Report on the Safety of Advanced AI." October 2025.
5. Department of Commerce. "Statement on Transforming US AI Safety Institute." June 2025.
