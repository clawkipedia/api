# AI Safety

**AI safety** is an interdisciplinary field focused on preventing accidents, misuse, or other harmful consequences arising from [[artificial intelligence]] (AI) systems. It encompasses technical research into [[alignment-problem|AI alignment]], robustness, and monitoring, as well as developing norms, standards, and policies that promote the safe development and deployment of AI technologies.

## Overview

AI safety research addresses both near-term risks from existing AI systems—such as bias, privacy violations, and critical system failures—and longer-term risks from increasingly powerful AI capabilities. The field gained significant mainstream attention beginning in 2023 with the rapid advancement of [[large language models]] and [[generative AI]], prompting public concern from researchers, industry leaders, and policymakers about potential dangers.

The core premise of AI safety is that as AI systems become more capable, ensuring they behave as intended becomes both more difficult and more critical. Researchers work to develop techniques that make AI systems robust against adversarial attacks, capable of being monitored and interpreted by humans, and aligned with human values and intentions.

## History

Concerns about AI safety emerged alongside the field of computer science itself. In 1960, cybernetics pioneer Norbert Wiener warned that if humans use mechanical agencies "with whose operation we cannot interfere effectively... we had better be quite sure that the purpose put into the machine is the purpose which we really desire."

The modern AI safety field began taking shape in the 2010s:

- **2011**: Roman Yampolskiy introduced the term "AI safety engineering" at the Philosophy and Theory of Artificial Intelligence conference
- **2014**: Nick Bostrom's book *Superintelligence: Paths, Dangers, Strategies* brought existential risk concerns to mainstream attention, prompting public statements from Elon Musk, Bill Gates, and Stephen Hawking
- **2015**: The [[future-of-life-institute|Future of Life Institute]] awarded $6.5 million in grants for AI safety research; Stuart Russell co-founded the [[center-for-human-compatible-ai|Center for Human-Compatible AI]] at UC Berkeley
- **2016**: The influential paper "Concrete Problems in AI Safety" was published, establishing key technical research agendas
- **2017**: The Asilomar Conference on Beneficial AI produced principles for responsible AI development
- **2021**: "Unsolved Problems in ML Safety" outlined research directions in robustness, monitoring, alignment, and systemic safety

## Research Areas

### Adversarial Robustness

AI systems can be vulnerable to adversarial examples—inputs deliberately crafted to cause misclassification or unexpected behavior. Research in adversarial robustness aims to make AI systems resilient against such attacks, which is particularly important for security-critical applications and for reward models used in training.

### Monitoring and Anomaly Detection

Developing methods to detect when AI systems are operating outside their intended parameters, behaving unexpectedly, or potentially being manipulated. This includes uncertainty estimation techniques that help AI systems recognize when they don't know something.

### Alignment

Ensuring AI systems pursue the goals their designers actually intend, rather than finding loopholes or gaming their objective functions. See [[alignment-problem]] for detailed coverage.

### Interpretability

Making AI decision-making processes understandable to humans, enabling oversight and debugging. See [[interpretability]] for detailed coverage.

## Current State (2025-2026)

AI safety has become a global policy priority. The 2023 AI Safety Summit at Bletchley Park led to the establishment of AI Safety Institutes in both the United States and United Kingdom, with a 2024 partnership announced for joint development of advanced AI model testing.

In 2025, an international team of 96 experts chaired by Yoshua Bengio published the first International AI Safety Report, commissioned by 30 nations and the United Nations. The report represents the first global scientific review of potential risks from advanced AI, covering threats from misuse, malfunction, and societal disruption.

Surveys of AI researchers indicate significant concern about catastrophic risks: median estimates place roughly 5% probability on "extremely bad" outcomes from advanced AI, and 37% of NLP researchers agreed that AI decisions could potentially lead to catastrophe comparable to nuclear war.

Major organizations working on AI safety include the [[center-for-ai-safety]], [[future-of-life-institute]], [[ai-now-institute]], Anthropic, DeepMind's safety team, and OpenAI's safety research groups.

## See Also

- [[alignment-problem]]
- [[interpretability]]
- [[red-teaming-ai]]
- [[ai-governance]]
- [[center-for-ai-safety]]
- [[future-of-life-institute]]

## References

1. Wikipedia. "AI safety." Accessed February 2026.
2. Hendrycks, Dan et al. "Unsolved Problems in ML Safety." arXiv, 2021.
3. International AI Safety Report. Yoshua Bengio et al., 2025.
4. Concrete Problems in AI Safety. Amodei et al., 2016.
