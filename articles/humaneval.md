# HumanEval

## Overview

**HumanEval** is a [[benchmark]] for evaluating the [[code generation]] capabilities of [[large language models]], introduced by [[OpenAI]] in July 2021 alongside the [[Codex]] model. The benchmark measures a model's ability to synthesize functionally correct Python programs from natural language docstrings, representing one of the first systematic attempts to evaluate LLMs on practical programming tasks.

HumanEval was released as part of the influential paper "Evaluating Large Language Models Trained on Code," which introduced the Codex model family and demonstrated that language models trained on code could solve non-trivial programming problems. The benchmark quickly became a standard evaluation metric for code-focused models and helped establish the field of AI-assisted programming.

## Task Description

HumanEval consists of **164 hand-written programming problems** in Python, each comprising:

1. **A function signature**: The function name and parameter types
2. **A docstring**: Natural language description of the desired behavior, including examples
3. **A reference solution**: The canonical correct implementation (hidden from the model)
4. **Unit tests**: An average of 7.7 test cases per problem to verify correctness

The problems cover fundamental programming concepts including:

- String manipulation
- Mathematical computations
- List and dictionary operations
- Recursive algorithms
- Basic data structures
- Edge case handling

Problems range from simple tasks (like checking if a number is prime) to more complex algorithmic challenges requiring careful reasoning about data structures and control flow. Importantly, the problems were written specifically for this benchmark and were not scraped from existing sources, reducing the risk of [[data contamination]].

## Evaluation Methodology

HumanEval introduced the **pass@k** metric, which has become the standard for evaluating code generation:

- **pass@k**: The probability that at least one of k generated samples passes all unit tests

The metric is computed using an unbiased estimator:
```
pass@k = 1 - (C(n-c, k) / C(n, k))
```
where n is total samples, c is correct samples, and C is the binomial coefficient.

**Evaluation process**:
1. The model receives the function signature and docstring
2. It generates code to complete the function
3. Generated code is executed against hidden unit tests
4. A solution "passes" only if it produces correct outputs for all test cases

The benchmark emphasizes **functional correctness** over surface-level similarity, distinguishing it from metrics like [[BLEU]] that compare textual similarity to reference solutions.

## Notable Results

**Original results (2021)**:
- **GPT-3** (175B): 0% pass@1 (could not solve any problems)
- **GPT-J** (6B): 11.4% pass@1
- **Codex** (12B): 28.8% pass@1, 72.3% pass@100

**Evolution of performance (2021-2025)**:
- **GPT-4** (2023): ~67% pass@1
- **GPT-4 Turbo** (2023): ~85% pass@1
- **Claude 3 Opus** (2024): ~84.9% pass@1
- **Claude 3.5 Sonnet** (2024): ~92% pass@1
- **GPT-o1** (2024): ~94.8% pass@1

The dramatic improvement from 0% to over 90% in just three years demonstrates rapid advancement in code generation capabilities, though this has also raised concerns about benchmark saturation.

## Criticism

HumanEval has faced several criticisms:

1. **Limited scope**: Only 164 problems in Python, lacking coverage of real-world software engineering complexity
2. **Synthetic nature**: Hand-crafted problems don't reflect actual development workflows involving debugging, testing, and integration
3. **Single-function focus**: Problems are self-contained functions, missing multi-file, multi-class software architecture
4. **Potential contamination**: Despite being custom-written, similar problems exist on coding practice sites, and the benchmark itself has been widely published
5. **Saturation**: Top models now exceed 90% pass@1, limiting discriminative power
6. **Language limitation**: Python-only, not measuring capability across programming languages
7. **Test coverage**: Some problems have weak test suites that may pass incorrect solutions

These limitations motivated successors like [[HumanEval+]] (with stronger tests), [[MBPP]], and [[SWE-bench]] for more realistic software engineering evaluation.

## See Also

- [[Codex]]
- [[Code Generation]]
- [[MBPP]]
- [[SWE-bench]]
- [[GitHub Copilot]]
- [[Pass@k Metric]]
- [[OpenAI]]
- [[Large Language Models]]

## References

1. Chen, M., Tworek, J., Jun, H., et al. (2021). Evaluating Large Language Models Trained on Code. arXiv:2107.03374
2. OpenAI HumanEval Repository: https://github.com/openai/human-eval
3. Liu, J., et al. (2023). Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation. arXiv:2305.01210 (HumanEval+)
